{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader # requires unstructured[local-inference]\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter, SpacyTextSplitter\n",
    "from IPython.display import Markdown\n",
    "\n",
    "CHUNK_SIZE = 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original RAG paper\n",
    "loader = UnstructuredPDFLoader('2005.11401.pdf', mode='single')\n",
    "loaded_doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_docs(doc_set, limit=5):\n",
    "    # display the first 'limit' documents in Markdown format with a different title for each chunk\n",
    "    for i, doc in enumerate(doc_set):\n",
    "        if i >= limit:\n",
    "            break\n",
    "        display(Markdown(f'#### Chunk {i}'))\n",
    "        # display the chunk with a border\n",
    "        display(Markdown(f'<div style=\"border: 1px solid orange; padding: 1px\">{doc.page_content}</div>'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Chunk 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
       "\n",
       "Patrick Lewis†‡, Ethan Perez(cid:63),\n",
       "\n",
       "1 2 0 2\n",
       "\n",
       "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
       "\n",
       "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
       "\n",
       "r p A 2 1\n",
       "\n",
       "†Facebook AI Research; ‡University College London; (cid:63)New York University; plewis@fb.com\n",
       "\n",
       "] L C . s c [\n",
       "\n",
       "4 v 1 0 4 1 1 . 5 0 0 2 : v i X r a\n",
       "\n",
       "Abstract\n",
       "\n",
       "Large pre-trained language models have been shown to store factual knowledge in their parameters,</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\"> and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We com- pare two R</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">AG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.\n",
       "\n",
       "1\n",
       "\n",
       "Introduction\n",
       "\n",
       "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\"> cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
       "\n",
       "supports\t(y)Question GenerationFact Veriﬁcation:Label GenerationDocumentIndex\n",
       "\n",
       "End-to-End Backprop through q and pθBarack\tObama\twasborn\tin\tHawaii.(x)\n",
       "\n",
       "Fact Veriﬁcation: Fact Query\n",
       "\n",
       "Margin-alize\n",
       "\n",
       "The\tDivineComedy\t(x)\n",
       "\n",
       "pθGenerator pθ(Parametric)\n",
       "\n",
       "This\t</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">14th\tcentury\tworkis\tdivided\tinto\t3sections:\t\"Inferno\",\"Purgatorio\"\t&\"Paradiso\"\t\t\t\t\t\t\t\t\t(y)\n",
       "\n",
       "The\tmiddle\tear\tincludesthe\ttympanic\tcavity\tandthe\tthree\tossicles.\t\t(y)Question Answering:Answer GenerationRetriever pη(Non-Parametric)z4z3z2z1d(z)Jeopardy QuestionGeneration:Answer Query\n",
       "\n",
       "qQueryEncoder\n",
       "\n",
       "q(x)\n",
       "\n",
       "MIPS\n",
       "\n",
       "Define\t\"middle\tear\"(x)Question Answering:Question Query\n",
       "\n",
       "Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder + Document Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For query x, we use Maximum Inner Product Search (MIPS) to ﬁnd the top-K documents zi. For</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Naive chunking\n",
    "simple_splitter = TokenTextSplitter.from_tiktoken_encoder(\"cl100k_base\", chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "docs = simple_splitter.transform_documents(loaded_doc)\n",
    "display_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "#### Chunk 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
       "\n",
       "Patrick Lewis†‡, Ethan Perez(cid:63),\n",
       "\n",
       "1 2 0 2\n",
       "\n",
       "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
       "\n",
       "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
       "\n",
       "r p A 2 1\n",
       "\n",
       "†Facebook AI Research; ‡University College London; (cid:63)New York University; plewis@fb.com\n",
       "\n",
       "] L C . s c [\n",
       "\n",
       "4 v 1 0 4 1 1 . 5 0 0 2 : v i X r a\n",
       "\n",
       "Abstract</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks. However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">Wikipedia, accessed with a pre-trained neural retriever. We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">1\n",
       "\n",
       "Introduction\n",
       "\n",
       "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47]. They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52]. While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">supports\t(y)Question GenerationFact Veriﬁcation:Label GenerationDocumentIndex\n",
       "\n",
       "End-to-End Backprop through q and pθBarack\tObama\twasborn\tin\tHawaii.(x)\n",
       "\n",
       "Fact Veriﬁcation: Fact Query\n",
       "\n",
       "Margin-alize\n",
       "\n",
       "The\tDivineComedy\t(x)\n",
       "\n",
       "pθGenerator pθ(Parametric)\n",
       "\n",
       "This\t14th\tcentury\tworkis\tdivided\tinto\t3sections:\t\"Inferno\",\"Purgatorio\"\t&\"Paradiso\"\t\t\t\t\t\t\t\t\t(y)\n",
       "\n",
       "The\tmiddle\tear\tincludesthe\ttympanic\tcavity\tandthe\tthree\tossicles.\t\t(y)Question Answering:Answer GenerationRetriever pη(Non-Parametric)z4z3z2z1d(z)Jeopardy QuestionGeneration:Answer Query\n",
       "\n",
       "qQueryEncoder\n",
       "\n",
       "q(x)\n",
       "\n",
       "MIPS</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recursive chunking\n",
    "contextual_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\"cl100k_base\", chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "docs = contextual_splitter.transform_documents(loaded_doc)\n",
    "display_docs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 233, which is longer than the specified 192\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 0"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
       "\n",
       "Patrick Lewis†‡, Ethan Perez(cid:63),\n",
       "\n",
       "1 2 0 2\n",
       "\n",
       "Aleksandra Piktus†, Fabio Petroni†, Vladimir Karpukhin†, Naman Goyal†, Heinrich Küttler†,\n",
       "\n",
       "Mike Lewis†, Wen-tau Yih†, Tim Rocktäschel†‡, Sebastian Riedel†‡, Douwe Kiela†\n",
       "\n",
       "r p\n",
       "\n",
       "A 2 1\n",
       "\n",
       "†Facebook AI Research; ‡University College London; (cid:63)New York University; plewis@fb.com\n",
       "\n",
       "] L C .\n",
       "\n",
       "s c\n",
       "\n",
       "[\n",
       "\n",
       "4 v 1 0 4 1 1 .\n",
       "\n",
       "5 0 0\n",
       "\n",
       "2\n",
       "\n",
       ":\n",
       "\n",
       "v\n",
       "\n",
       "i</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">X r a\n",
       "\n",
       "Abstract\n",
       "\n",
       "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down- stream NLP tasks.\n",
       "\n",
       "However, their ability to access and precisely manipulate knowl- edge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures.\n",
       "\n",
       "Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems.\n",
       "\n",
       "Pre- trained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks.\n",
       "\n",
       "We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation (RAG) — models which combine pre-trained parametric and non-parametric mem- ory for language generation.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever.\n",
       "\n",
       "We com- pare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token.\n",
       "\n",
       "We ﬁne-tune and evaluate our models on a wide range of knowledge- intensive NLP tasks and set the state of the art on three open domain QA tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures.\n",
       "\n",
       "For language generation tasks, we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">1\n",
       "\n",
       "Introduction\n",
       "\n",
       "Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl- edge from data [47].\n",
       "\n",
       "They can do so without any access to an external memory, as a parameterized implicit knowledge base [51, 52].\n",
       "\n",
       "While this development is exciting, such models do have down- sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into their predictions, and may produce “hallucinations” [38].\n",
       "\n",
       "Hybrid models that combine parametric memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these issues because knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.\n",
       "\n",
       "REALM\n",
       "\n",
       "[20] and ORQA</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Chunk 4"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<div style=\"border: 1px solid orange; padding: 1px\">[31], two recently introduced models that combine masked language models [8] with a differentiable retriever, have shown promising results,\n",
       "\n",
       "supports\t(y)Question GenerationFact Veriﬁcation:Label GenerationDocumentIndex\n",
       "\n",
       "End-to-End Backprop through q and pθBarack\tObama\twasborn\tin\tHawaii.(x)\n",
       "\n",
       "\n",
       "\n",
       "Fact Veriﬁcation: Fact Query\n",
       "\n",
       "Margin-alize\n",
       "\n",
       "The\tDivineComedy\t(x)\n",
       "\n",
       "pθGenerator pθ(Parametric)\n",
       "\n",
       "\n",
       "\n",
       "This\t14th\tcentury\tworkis\tdivided\tinto\t3sections:\t\"Inferno\",\"Purgatorio\"\t&\"Paradiso\"\t\t\t\t\t\t\t\t\t(y)\n",
       "\n",
       "\n",
       "\n",
       "The\tmiddle\tear\tincludesthe\ttympanic\tcavity\tandthe\tthree\tossicles.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SpaCy chunking\n",
    "contextual_splitter = SpacyTextSplitter.from_tiktoken_encoder(\"cl100k_base\", chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "docs = contextual_splitter.transform_documents(loaded_doc)\n",
    "display_docs(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
